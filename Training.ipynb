{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.12.3)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: lxml in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (5.3.0)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.3.1)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.9.0.post1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (4.46.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (4.67.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (1.14.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (0.26.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from faiss-cpu) (2.1.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.9.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "%pip install beautifulsoup4 lxml sentence-transformers faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "doc_root = r\"D:\\thesis\\PyPore3D_documentation\\PyPore3D_documentation\\functions\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract text from html files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function for reading html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_html(html_file):\n",
    "    with open(html_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        soup = BeautifulSoup(file, \"lxml\")\n",
    "        # Extract visible text from the body tag\n",
    "        return soup.get_text(separator=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for traversing the files and reading the html docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found file: p3dBlobPy.py_p3dAnisotropyAnalysis.html\n",
      "Found file: p3dBlobPy.py_p3dBasicAnalysis.html\n",
      "Found file: p3dBlobPy.py_p3dBlobAnalysis.html\n",
      "Found file: p3dBlobPy.py_p3dBlobLabeling.html\n",
      "Found file: p3dBlobPy.py_p3dBlobLabeling_8.html\n",
      "Found file: p3dBlobPy.py_p3dChamferDT.html\n",
      "Found file: p3dBlobPy.py_p3dGetMaxVolumeBlob3D.html\n",
      "Found file: p3dBlobPy.py_p3dGetMinVolumeBlob3D.html\n",
      "Found file: p3dBlobPy.py_p3dMinVolumeFilter3D.html\n",
      "Found file: p3dBlobPy.py_p3dMorphometricAnalysis.html\n",
      "Found file: p3dBlobPy.py_p3dSquaredEuclideanDT.html\n",
      "Found file: p3dBlobPy.py_p3dTextureAnalysis.html\n",
      "Found file: p3dFiltPy.py_p3dAnisotropicDiffusionFilter16.html\n",
      "Found file: p3dFiltPy.py_p3dAnisotropicDiffusionFilter8.html\n",
      "Found file: p3dFiltPy.py_p3dAutoThresholding16.html\n",
      "Found file: p3dFiltPy.py_p3dAutoThresholding8.html\n",
      "Found file: p3dFiltPy.py_p3dBilateralFilter16.html\n",
      "Found file: p3dFiltPy.py_p3dBilateralFilter8.html\n",
      "Found file: p3dFiltPy.py_p3dBoinHaibelRingRemover8.html\n",
      "Found file: p3dFiltPy.py_p3dClearBorderFilter8.html\n",
      "Found file: p3dFiltPy.py_p3dGaussianFilter16.html\n",
      "Found file: p3dFiltPy.py_p3dGaussianFilter8.html\n",
      "Found file: p3dFiltPy.py_p3dMeanFilter16.html\n",
      "Found file: p3dFiltPy.py_p3dMeanFilter8.html\n",
      "Found file: p3dFiltPy.py_p3dMedianFilter16.html\n",
      "Found file: p3dFiltPy.py_p3dMedianFilter8.html\n",
      "Found file: p3dFiltPy.py_p3dReadRaw16.html\n",
      "Found file: p3dFiltPy.py_p3dReadRaw8.html\n",
      "Found file: p3dFiltPy.py_p3dSijbersPostnovRingRemover16.html\n",
      "Found file: p3dFiltPy.py_p3dSijbersPostnovRingRemover8.html\n",
      "Found file: p3dFiltPy.py_p3dWriteRaw16.html\n",
      "Found file: p3dFiltPy.py_p3dWriteRaw8.html\n",
      "Found file: p3dFiltPy_16.py_p3dAnisotropicDiffusionFilter16.html\n",
      "Found file: p3dFiltPy_16.py_p3dAutoThresholding16.html\n",
      "Found file: p3dFiltPy_16.py_p3dBilateralFilter16.html\n",
      "Found file: p3dFiltPy_16.py_p3dGaussianFilter16.html\n",
      "Found file: p3dFiltPy_16.py_p3dMeanFilter16.html\n",
      "Found file: p3dFiltPy_16.py_p3dMedianFilter16.html\n",
      "Found file: p3dFiltPy_16.py_p3dReadRaw16.html\n",
      "Found file: p3dFiltPy_16.py_p3dSijbersPostnovRingRemover16.html\n",
      "Found file: p3dFiltPy_16.py_p3dWriteRaw16.html\n",
      "Found file: p3dSITKPy.py_p3d_ConnectedThresholdImageFilter.html\n",
      "Found file: p3dSITKPy.py_p3d_CurvatureAnisotropicDiffusionImageFilter.html\n",
      "Found file: p3dSITKPy.py_p3d_CurvatureFlowImageFilter.html\n",
      "Found file: p3dSITKPy.py_p3d_Dilate.html\n",
      "Found file: p3dSITKPy.py_p3d_Erode.html\n",
      "Found file: p3dSITKPy.py_p3d_GradientAnisotropicDiffusionImageFilter.html\n",
      "Found file: p3dSITKPy.py_p3d_GradientMagnitudeImageFilter.html\n",
      "Found file: p3dSITKPy.py_p3d_HMinimaFilter.html\n",
      "Found file: p3dSITKPy.py_p3d_IsolatedConnectedImageFilter.html\n",
      "Found file: p3dSITKPy.py_p3d_KMeansClustering.html\n",
      "Found file: p3dSITKPy.py_p3d_MinMaxCurvatureFlowImageFilter.html\n",
      "Found file: p3dSITKPy.py_p3d_MultiThresholding.html\n",
      "Found file: p3dSITKPy.py_p3d_NeighborhoodConnectedImageFilter.html\n",
      "Found file: p3dSITKPy.py_p3d_WatershedSegmentation.html\n",
      "Found file: p3dSITKPy.py_p3d_WatershedSegmentation_16.html\n",
      "Found file: p3dSITKPy_16.py_p3d_HMinimaFilter.html\n",
      "Found file: p3dSITKPy_16.py_p3d_WatershedSegmentation.html\n",
      "Number of processed HTML files: 58\n",
      "Sample extracted text from first file: PyPore3D p3dFiltPy p3dBlobPy p3dSkelPy p3dSITKPy p3dFiltPy_16 p3dSITKPy_16 p3dFiltPy.py_p3dReadRaw8 p3dFiltPy.py_p3dWriteRaw8 p3dFiltPy.py_p3dGaussianFilter8 p3dFiltPy.py_p3dMeanFilter8 p3dFiltPy.py_p...\n"
     ]
    }
   ],
   "source": [
    "doc_chunks = []\n",
    "\n",
    "# Traverse the folder and process files with .html extensions\n",
    "for root, dirs, files in os.walk(doc_root):\n",
    "    for file in files:\n",
    "        print(f\"Found file: {file}\")\n",
    "        # Check for .html extension\n",
    "        if file.lower().endswith(\".html\"):  \n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    # Parse HTML using BeautifulSoup\n",
    "                    soup = BeautifulSoup(f, \"lxml\")\n",
    "                    \n",
    "                    # Extract text from the body of the HTML file\n",
    "                    body_text = soup.body.get_text(separator=\" \", strip=True) if soup.body else \"\"\n",
    "                    \n",
    "                    # Ensure non-empty content is stored\n",
    "                    if body_text:\n",
    "                        doc_chunks.append(body_text)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process {file_path}: {e}\")\n",
    "\n",
    "# Output to confirm extraction\n",
    "print(f\"Number of processed HTML files: {len(doc_chunks)}\")\n",
    "if doc_chunks:\n",
    "    print(f\"Sample extracted text from first file: {doc_chunks[0][:200]}...\")\n",
    "else:\n",
    "    print(\"No valid HTML files were processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    lines = text.splitlines()\n",
    "    # Remove empty lines and trim whitespace\n",
    "    return \"\\n\".join(line.strip() for line in lines if line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_chunks = [clean_text(chunk) for chunk in doc_chunks]\n",
    "if len(doc_chunks) == 0:\n",
    "    raise ValueError(\"No valid text extracted from the HTML files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faiss Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = embedder.encode(doc_chunks, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embeddings: torch.Size([58, 384])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of embeddings: {embeddings.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating and Populating the FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of document chunks: 58\n",
      "First document chunk: PyPore3D p3dFiltPy p3dBlobPy p3dSkelPy p3dSITKPy p3dFiltPy_16 p3dSITKPy_16 p3dFiltPy.py_p3dReadRaw8 p3dFiltPy.py_p3dWriteRaw8 p3dFiltPy.py_p3dGaussianFilter8 p3dFiltPy.py_p3dMeanFilter8 p3dFiltPy.py_p3dMedianFilter8 p3dFiltPy.py_p3dAnisotropicDiffusionFilter8 p3dFiltPy.py_p3dBilateralFilter8 p3dFiltPy.py_p3dSijbersPostnovRingRemover8 p3dFiltPy.py_p3dClearBorderFilter8 p3dFiltPy.py_p3dAutoThresholding8 p3dFiltPy_16.py_p3dReadRaw16 p3dFiltPy_16.py_p3dWriteRaw16 p3dFiltPy_16.py_p3dAutoThresholding16 p3dFiltPy_16.py_p3dGaussianFilter16 p3dFiltPy_16.py_p3dMeanFilter16 p3dFiltPy_16.py_p3dMedianFilter16 p3dFiltPy_16.py_p3dAnisotropicDiffusionFilter16 p3dFiltPy_16.py_p3dBilateralFilter16 p3dFiltPy_16.py_p3dSijbersPostnovRingRemover16 p3dBlobPy.py_p3dBasicAnalysis p3dBlobPy.py_p3dAnisotropyAnalysis p3dBlobPy.py_p3dBlobLabeling p3dBlobPy.py_p3dGetMaxVolumeBlob3D p3dBlobPy.py_p3dGetMinVolumeBlob3D p3dBlobPy.py_p3dChamferDT p3dBlobPy.py_p3dMorphometricAnalysis p3dBlobPy.py_p3dTextureAnalysis p3dBlobPy.py_p3dMinVolumeFilter3D p3dBlobPy.py_p3dBlobAnalysis p3dFiltPy.py_p3dReadRaw8 p3dFiltPy.py_p3dWriteRaw8 p3dFiltPy.py_p3dGaussianFilter8 p3dFiltPy.py_p3dMeanFilter8 p3dFiltPy.py_p3dMedianFilter8 p3dFiltPy.py_p3dAnisotropicDiffusionFilter8 p3dFiltPy.py_p3dBilateralFilter8 p3dFiltPy.py_p3dBoinHaibelRingRemover8 p3dFiltPy.py_p3dSijbersPostnovRingRemover8 p3dFiltPy.py_p3dClearBorderFilter8 p3dFiltPy.py_p3dAutoThresholding8 p3dSITKPy.py_p3d_Dilate p3dSITKPy.py_p3d_Erode p3dSITKPy.py_p3d_HMinimaFilter p3dSITKPy.py_p3d_MultiThresholding p3dSITKPy.py_p3d_NeighborhoodConnectedImageFilter p3dSITKPy.py_p3d_ConnectedThresholdImageFilter PyPore3D » p3dBlobPy.py_p3dAnisotropyAnalysis View page source p3dBlobPy.py_p3dAnisotropyAnalysis  p3dBlobPy. py_p3dAnisotropyAnalysis ( image_data , dimx , dimy , dimz = 0 , resolution = 1.0 ) [source]  Returns a struct of parameters computed from input binary image. Computed parameters are [1]: Result = py_p3dAnisotropyAnalysis ( image_data, dimx, dimy, [dimz = value] [, resolution = value] [, details = boolean] ) Performs a series of direct 3D basic analysis for randomly distributed porous media. Computed parameters are [1]: I [-]: sotropy index. It measures the similarity of a fabric to a uniform distribution and varies between 0 (all observation  confined to a single plane or axis) and 1 (perfect isotropy) E [-]: Elongation index. It measures the preferred orientation of a fabric in the u1/u2 plane and varies between 0 (no  preferred orientation) and 1 (a perfect preferred orientation with all observations parallel). image_data: A 2D or 3D matrix of type BYTE  representing the input image to write to disk. dimx,dimy,dimz: three variables representing the dimensions of image to read. resolution: A decimal value representing the resolution of input image. If this value is not specified a voxelsize of 1.0 is  assumed that means output values are expressed in voxel unit. [1] S.C. Cowin and A.J. Laborde, The relationship between the elasticity tensor and the fabric tensor. Mechanics of Materials,   Vol. 4, No. 22, pp. 137-147, 1985. [2] S.C. Cowin, Wolff’s law of trabecular architecture at remodeling equilibrium. Journal of Biomechanical Engineering, Vol.  108, No. 1, pp. 83-88, 1986. [3] W.J. Whitehouse, The quantitative morphology of anisotropic trabecular bone. Journal of Microscoscopy, Vol. 101, pp. 153-168, 1974. [4] T.P. Harrigan and R.W. Mann, Characterization of microstructural anisotropy in orthotropic materials using a second rank  tensor. Journal of Material Science, Vol. 19, No. 3, pp. 761-767, 1984. [5] D. I. Benn, Fabric shape and the interpolation of sedimentary fabric data. Journal of Sedimentary Research, Vol. 64, No.    2, pp. 910-915, 1994. Previous Next © Copyright 2021, Amal Aboulhassan. Built with Sphinx using a theme provided by Read the Docs .\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of document chunks: {len(doc_chunks)}\")\n",
    "if len(doc_chunks) > 0:\n",
    "    print(f\"First document chunk: {doc_chunks[0]}\")\n",
    "else:\n",
    "    print(\"No document chunks found! Check your HTML files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_np = embeddings.detach().cpu().numpy()\n",
    "dimension = embeddings_np.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = \"\"\n",
    "def search_documentation(query, index, chunks, embedder, top_k=3):\n",
    "    query_embedding = embedder.encode([query])\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    results = [chunks[i] for i in indices[0]]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codegemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "Raw Response Text: {\"model\":\"codegemma\",\"created_at\":\"2024-12-30T02:00:31.9148334Z\",\"response\":\"\",\"done\":true,\"done_reason\":\"load\"}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "url = \"http://localhost:11434/api/generate\"\n",
    "payload = {\n",
    "    \"model\": \"codegemma\",\n",
    "    \"prompt\": results\n",
    "}\n",
    "response = requests.post(url, json=payload)\n",
    "print(\"Status Code:\", response.status_code)\n",
    "    \n",
    "raw_text = response.text\n",
    "print(\"Raw Response Text:\", raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated Result: \n"
     ]
    }
   ],
   "source": [
    "lines = raw_text.strip().split(\"\\n\")\n",
    "words = []\n",
    "for line in lines:\n",
    "    try:\n",
    "        json_obj = json.loads(line)\n",
    "        words.append(json_obj.get(\"response\", \"\"))\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Failed to parse line: {line}\")\n",
    "\n",
    "# Concatenate all words\n",
    "result = \"\".join(words)\n",
    "\n",
    "print(\"Concatenated Result:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated Result: **p3dFiltPy_16** is a Python module developed by the **Princeton University Human Cognitive Neuroscience Lab (Princeton HCN Lab)**. It is specifically designed for **processing and analyzing fMRI data** acquired using the **fMRIprep software**.\n",
      "\n",
      "**Purpose of p3dFiltPy_16:**\n",
      "\n",
      "* Provides functions for preprocessing and filtering fMRI data, including:\n",
      "    * Motion correction\n",
      "    * Artifact removal\n",
      "    * Spatial smoothing\n",
      "* Facilitates the application of various statistical models to fMRI data, such as:\n",
      "    * Linear mixed-effects models (LME)\n",
      "    * General linear models (GLM)\n",
      "* Enables the extraction of statistical estimates, including contrasts and effect sizes.\n",
      "\n",
      "**Key Features:**\n",
      "\n",
      "* Uses the **p3dfilt library**, which provides efficient implementations of common fMRI preprocessing and analysis techniques.\n",
      "* Integrates with the **fMRIprep pipeline**, allowing for seamless data preprocessing within the pipeline framework.\n",
      "* Offers a range of options for customization, including filter parameters, model settings, and contrast definitions.\n",
      "* Provides detailed documentation and examples, making it easy to get started with p3dFiltPy_16.\n",
      "\n",
      "**Applications:**\n",
      "\n",
      "p3dFiltPy_16 is widely used in the field of cognitive neuroscience for the following tasks:\n",
      "\n",
      "* Studying brain activity during various cognitive tasks\n",
      "* Investigating neural mechanisms of cognitive processes\n",
      "* Identifying brain regions involved in specific cognitive functions\n",
      "\n",
      "**Benefits of Using p3dFiltPy_16:**\n",
      "\n",
      "* Efficient and accurate fMRI data preprocessing and analysis\n",
      "* Comprehensive set of features for fMRI analysis\n",
      "* Easy integration with the fMRIprep pipeline\n",
      "* Comprehensive documentation and support\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "p3dFiltPy_16 is an invaluable tool for researchers working with fMRI data. It provides a powerful and flexible platform for preprocessing, filtering, and analyzing fMRI data, enabling comprehensive insights into brain activity during cognitive tasks.\n"
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "    \"model\": \"codegemma\",\n",
    "    \"prompt\": \"can you explain to me what p3dFiltPy_16 is\"\n",
    "}\n",
    "response = requests.post(url, json=payload)\n",
    "raw_text = response.text\n",
    "lines = raw_text.strip().split(\"\\n\")\n",
    "words = []\n",
    "for line in lines:\n",
    "    try:\n",
    "        json_obj = json.loads(line)\n",
    "        words.append(json_obj.get(\"response\", \"\"))\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Failed to parse line: {line}\")\n",
    "\n",
    "# Concatenate all words\n",
    "result = \"\".join(words)\n",
    "\n",
    "print(\"Concatenated Result:\", result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
